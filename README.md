# Predicting Customer Churn From Reviews Using Transformers

## üìå Project Overview

Businesses receive thousands of customer reviews every day, but it‚Äôs difficult to manually identify which customers might be at risk of leaving. Traditional keyword-based or rule-based methods miss subtle cues like *‚Äúmight try somewhere else‚Äù* versus *‚Äúconsidering alternatives.‚Äù*

This project uses Natural Language Processing (NLP) to predict churn risk directly from review text using two models:

1. **TF-IDF + Logistic Regression** (baseline)
2. **Fine-tuned DistilBERT Transformer** (advanced)

Beyond prediction, the project incorporates **interpretability**, including attention visualizations, word importance, and error analysis, to better understand *why* the model makes its predictions.

This model aligns closely with course material‚Äîtransformer architecture, model fine-tuning, bias analysis, and evaluation metrics‚Äîand applies them to a realistic business problem.

---

## üöÄ Key Features

- Two full modeling pipelines (TF-IDF baseline + Transformer)
- High predictive performance (DistilBERT AUC ‚âà 0.984)
- Complete evaluation suite: Accuracy, Precision, Recall, F1, AUC
- Interpretability tools: Attention maps, word clouds, calibration curves
- Error analysis: Where models disagree, where they fail
- Saved models + inference functions for deploying predictions

---

## üß† Problem Statement

Companies struggle to track churn risk manually across large volumes of reviews. Sentiment alone does not always indicate churn, and subtle signals can easily be missed without contextual modeling.

**Objective:**  
Build a binary classifier that identifies churn risk from review text and outperforms a strong TF-IDF baseline. The model also highlights which phrases indicate churn, helping teams intervene early.

**Target Goals:**
- AUC ‚â• 0.85  
- Precision ‚â• 70% for top 10% high-risk predictions  
- Inference latency < 100ms  

---

## üîó Connection to Course Concepts

This project incorporates several course concepts:

### üß© Transformers & Self-Attention
- Fine-tuning DistilBERT for classification  
- Understanding contextual embeddings  
- Visualizing attention weights  

### üìä Evaluation Metrics
- ROC/AUC  
- Precision, Recall, F1  
- Probability calibration curves  
- Confusion matrix analysis  

### üõ† Machine Learning Workflow
- Data preprocessing  
- Baseline model development  
- Model comparison  
- Hyperparameter tuning  
- Error and bias analysis  

### ‚öñÔ∏è Ethical AI
- Bias in language models  
- Reviewer bias and dialect variation  
- Limitations of using sentiment to infer true churn  

---

## üìÇ Repository Structure

‚îú‚îÄ‚îÄ churn_model/
‚îÇ ‚îú‚îÄ‚îÄ distilbert/ # Fine-tuned DistilBERT model + tokenizer
‚îÇ ‚îú‚îÄ‚îÄ lr_model.pkl # Logistic Regression model
‚îÇ ‚îî‚îÄ‚îÄ tfidf_vectorizer.pkl # TF-IDF vectorizer vocabulary
‚îÇ
‚îú‚îÄ‚îÄ outputs/
‚îÇ ‚îú‚îÄ‚îÄ attention_example_1.png
‚îÇ ‚îú‚îÄ‚îÄ attention_example_2.png
‚îÇ ‚îú‚îÄ‚îÄ attention_example_3.png
‚îÇ ‚îú‚îÄ‚îÄ calibration_curve.png
‚îÇ ‚îú‚îÄ‚îÄ confusion_matrices.png
‚îÇ ‚îú‚îÄ‚îÄ model_comparison.png
‚îÇ ‚îú‚îÄ‚îÄ roc_curves.png
‚îÇ ‚îú‚îÄ‚îÄ word_importance.png
‚îÇ ‚îî‚îÄ‚îÄ training_summary.json
‚îÇ
‚îú‚îÄ‚îÄ yelp_churn_classification.ipynb
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

yaml
Copy code

---

## üìä Dataset

**Dataset:** Yelp Polarity (20,000-sample subset)  
**Source:** https://huggingface.co/datasets/yelp_polarity  
**License:** CC BY 4.0  

### Original Labels  
- 0 ‚Üí negative (1‚Äì2 stars)  
- 1 ‚Üí positive (4‚Äì5 stars)

### Churn Mapping (Project)  
- **Churn (1)** ‚Üí negative reviews  
- **No Churn (0)** ‚Üí positive reviews  

**Distribution:** Balanced (‚âà50/50), stratified across splits.

---

## üìò Data Card

| Field | Details |
|-------|---------|
| **Dataset** | Yelp Polarity (20k sample) |
| **License** | CC BY 4.0 |
| **Features** | `text`, `label` |
| **Task** | Binary churn classification |
| **Processing** | Tokenization, label remapping, stratified split |
| **Risks** | Reviewer bias, slang/dialect bias, sarcasm, extreme opinions |
| **Limitations** | No metadata, no behavioral churn data, English-only |

---

## ü§ñ Model Card ‚Äî DistilBERT Churn Classifier

### Overview
- Base Model: `distilbert-base-uncased`
- Architecture: 6-layer transformer encoder
- Parameters: ~66M
- Pretrained on large English corpus

### Fine-Tuning Configuration
- Max length: 256 tokens  
- Batch size: 16  
- Epochs: 3  
- Learning rate: 2e-5  
- Warmup: 500 steps  
- Optimizer: AdamW  
- Early stopping enabled  

### Performance (Test Set)

| Metric | Score |
|--------|--------|
| **Accuracy** | 0.9367 |
| **Precision** | 0.9422 |
| **Recall** | 0.9323 |
| **F1 Score** | 0.9372 |
| **AUC** | 0.9838 |

### Intended Use
- Academic projects  
- Research prototyping  
- NLP demonstration and teaching  

### Not Intended For
- Automated decisions without human review  
- Financial, legal, or hiring workflows  
- Commercial deployment without validation  

**License:** Apache 2.0  

---

## üìú Licenses

- **DistilBERT:** Apache 2.0  
- **Scikit-Learn:** BSD 3-Clause  
- **Yelp Polarity Dataset:** CC BY 4.0  
- **Project Code:** MIT License  

---

## ‚öñÔ∏è Ethical & Bias Considerations

### Potential Issues
- **Reviewer Bias:** Opinions depend on personal, cultural, or social influences  
- **Language Bias:** Transformers may misinterpret slang or non-standard English  
- **Sentiment vs. Behavior:** Negative sentiment isn‚Äôt always churn  
- **Model Bias:** Training data may amplify certain language patterns  

### Mitigation
- Attention maps for transparency  
- Word importance to check model overreliance  
- Error analysis to identify systematic mistakes  
- Clear disclaimers about the model‚Äôs limitations  

### Responsible Use Guidance  
This system should **support**, not replace, human judgment. It should not be used to automatically penalize or target customers.

---

## ‚öôÔ∏è Methodology

### üîπ 1. Baseline: TF-IDF + Logistic Regression
- Vectorizer: 10,000 features  
- n-grams: (1,2)  
- Logistic Regression (balanced class weights)  
- Purpose: Establish a traditional text classification baseline  

### üîπ 2. Transformer: Fine-Tuned DistilBERT
- Tokenization: WordPiece  
- Context-aware embeddings  
- Self-attention mechanism to capture long-range meaning  
- Early stopping + validation monitoring  
- Purpose: Capture nuance that baseline misses  

### Why Transformers?
Transformers understand:
- Negation (e.g., *‚Äúnot terrible‚Äù*)  
- Sarcasm  
- Mixed sentiment  
- Subtle dissatisfaction cues  

---

## üìà Results Summary

### Test Performance Comparison

| Metric | TF-IDF + LR | DistilBERT | Improvement |
|--------|-------------|------------|-------------|
| Accuracy | 0.9153 | **0.9367** | +2.1% |
| Precision | 0.9122 | **0.9422** | +3.3% |
| Recall | 0.9218 | **0.9323** | +1.1% |
| F1 Score | 0.9169 | **0.9372** | +2.2% |
| AUC | 0.9731 | **0.9838** | +1.1% |

### Key Takeaways
- DistilBERT consistently outperforms the classical baseline  
- Strong improvements in precision and F1  
- Better handling of nuance and ambiguous reviews  
- High baseline limits amount of possible AUC improvement  

---

## üîç Interpretability & Analysis

Included tools:
- Confusion matrices  
- ROC curves  
- Calibration curves  
- TF-IDF coefficient importance  
- Attention heatmaps  
- Error breakdown:
  - Baseline wrong / BERT correct  
  - BERT wrong / baseline correct  
  - Both wrong (hard cases)  

Observed patterns:
- Churn cues: ‚Äúterrible,‚Äù ‚Äúworst,‚Äù ‚Äúnever returning,‚Äù ‚Äúrude‚Äù  
- Retention cues: ‚Äúlove,‚Äù ‚Äúexcellent,‚Äù ‚Äúamazing,‚Äù ‚Äúperfect‚Äù  

---

## üß™ How to Run

### Install Requirements
pip install -r requirements.txt

shell
Copy code

### Run Notebook
jupyter notebook yelp_churn_classification.ipynb

yaml
Copy code

### Google Colab
- Upload notebook  
- Runtime ‚Üí Change Runtime Type ‚Üí GPU  
- Run all  

---

## üîÆ Inference

### Baseline Example
```python
predict_churn_baseline("This place was awful. Not coming back.")
DistilBERT Example
python
Copy code
predict_churn_bert("Amazing service! Loved this place.")
üß≠ Critical Analysis
Impact
This project shows how NLP can support early churn detection by surfacing dissatisfied customers at scale. It adds transparency through interpretability techniques and connects transformer theory directly to a meaningful real-world use case.

What It Reveals
Transformers capture nuance beyond simple sentiment

Interpretability tools help validate model reasoning

Even with strong baselines, transformers provide measurable gains

Limitations
Yelp reviews don‚Äôt reflect true churn behavior

Model trained only on English

No temporal or user-level information

Clean dataset may inflate performance

Next Steps
Add RoBERTa and LoRA/QLoRA

Integrate SHAP explanations

Build Streamlit demo

Add more diverse multi-platform review data

üèÅ Conclusion
This project demonstrates how transformer-based NLP models can meaningfully improve churn prediction from text while providing transparent and interpretable explanations. DistilBERT achieves strong performance, outperforming a competitive TF-IDF baseline across all metrics, and aligns with technical and ethical concepts discussed in the course.

Author: Ashley Stevens
Course: LLM Bootcamp
Date: November 2024
